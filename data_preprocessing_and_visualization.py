# -*- coding: utf-8 -*-
"""DATA PREPROCESSING AND VISUALIZATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/143-Wyvo-LL9tv-nWB5pp2a1wMPRTvzth
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk  # Import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from wordcloud import WordCloud
import os

# Download stopwords from nltk
nltk.download('stopwords')

# Load the dataset
def load_data(file_path):
    df = pd.read_csv(file_path)
    return df

# Clean the dataset
def clean_data(df):
    # Print the column names for debugging
    print("Columns in the dataset:", df.columns.tolist())

    # Remove duplicates
    df.drop_duplicates(inplace=True)

    # Handle missing values
    df.dropna(subset=['description'], inplace=True)  # Drop rows where description is NaN
    df['description'].fillna('', inplace=True)  # Fill missing descriptions with empty strings

    # Generate a placeholder label for demonstration
    df['label'] = np.random.choice(['Category 1', 'Category 2'], size=len(df))  # Example categories

    return df

# Text preprocessing
def preprocess_text(text):
    # Lowercase
    text = text.lower()

    # Remove punctuation
    text = re.sub(f'[{string.punctuation}]', ' ', text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    text = ' '.join(word for word in text.split() if word not in stop_words)

    # Stemming (using Porter Stemmer)
    porter = PorterStemmer()
    text = ' '.join(porter.stem(word) for word in text.split())

    return text

# Apply preprocessing to the dataset
def preprocess_data(df):
    df['description'] = df['description'].apply(preprocess_text)
    return df

# Address class imbalance
def handle_imbalance(X, y):
    # Count the number of instances in each class
    class_counts = y.value_counts()
    print("Class distribution before handling imbalance:\n", class_counts)

    # Resampling using SMOTE
    smote = SMOTE()
    X_resampled, y_resampled = smote.fit_resample(X, y)

    return X_resampled, y_resampled

# Exploratory Data Analysis
def perform_eda(df, source):
    print(f"Performing EDA for {source}")
    print("Summary statistics:\n", df.describe())

    # Visualize class distribution
    plt.figure(figsize=(10, 6))
    sns.countplot(x='label', data=df)
    plt.title(f'Class Distribution for {source}')
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.show()

    # Visualize word cloud
    plt.figure(figsize=(10, 8))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(df['description']))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

# Main function to run the processing on multiple datasets
def main(file_paths):
    for file_path in file_paths:
        source = os.path.basename(file_path).split('.')[0]  # Get the source name from the filename
        print(f"Processing dataset: {source}")

        df = load_data(file_path)
        df = clean_data(df)
        df = preprocess_data(df)

        # Split data into features and target variable before handling imbalance
        X = df['description']
        y = df['label']

        # Vectorization (using TF-IDF)
        vectorizer = TfidfVectorizer()
        X_vectorized = vectorizer.fit_transform(X)

        # Handle class imbalance after vectorization
        X_resampled, y_resampled = handle_imbalance(X_vectorized, y)

        # Perform EDA
        perform_eda(df, source)

        print(f"Data preprocessing complete for {source}. Ready for model development.")

if __name__ == '__main__':
    # Specify the paths to your datasets
    file_paths = [
        '/content/sample_data/Al Jazeera_articles.csv',
        '/content/sample_data/BBC_articles.csv',
        '/content/sample_data/CNN_articles.csv',
        '/content/sample_data/New York Times_articles.csv',
        '/content/sample_data/The Guardian_articles.csv'
    ]
    main(file_paths)