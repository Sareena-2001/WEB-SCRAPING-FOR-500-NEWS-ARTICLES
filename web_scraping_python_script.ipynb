{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import threading\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Time limit in seconds for each thread\n",
        "TIME_LIMIT = 60\n",
        "\n",
        "def fetch_articles(source, url):\n",
        "    print(f\"Fetching articles from: {url}\")\n",
        "\n",
        "    article_data = []\n",
        "    max_articles = 500\n",
        "    start_time = time.time()  # Record the start time\n",
        "\n",
        "    try:\n",
        "        while len(article_data) < max_articles:\n",
        "            # Check if the elapsed time exceeds the TIME_LIMIT\n",
        "            elapsed_time = time.time() - start_time\n",
        "            if elapsed_time > TIME_LIMIT:\n",
        "                print(f\"Time limit reached for {source}. Stopping early.\")\n",
        "                break\n",
        "\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            feed = feedparser.parse(response.content)\n",
        "\n",
        "            articles = feed.entries\n",
        "\n",
        "            if not articles:\n",
        "                print(f\"No more articles available from {source}.\")\n",
        "                break\n",
        "\n",
        "            # Append articles until the limit is reached\n",
        "            for article in articles:\n",
        "                if len(article_data) >= max_articles:  # Stop if we have enough articles\n",
        "                    print(f\"Reached 500 articles for {source}.\")\n",
        "                    break\n",
        "                article_info = {\n",
        "                    'title': article.title,\n",
        "                    'link': article.link,\n",
        "                    'published': getattr(article, 'published', 'No date available'),\n",
        "                    'description': article.get('description', 'No description available')\n",
        "                }\n",
        "                article_data.append(article_info)\n",
        "\n",
        "            print(f\"Fetched {len(article_data)} articles from {source}. Total collected: {len(article_data)}\")\n",
        "            time.sleep(1)  # Sleep to avoid overwhelming the server\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching articles from {url}: {e}\")\n",
        "\n",
        "    # Save to CSV regardless of whether it's interrupted or completed\n",
        "    df = pd.DataFrame(article_data)\n",
        "    df.to_csv(f'{source}_articles.csv', index=False)\n",
        "    print(f\"Saved articles from {source} to {source}_articles.csv\")\n",
        "\n",
        "def controlScraping(websites):\n",
        "    threads = []\n",
        "    try:\n",
        "        for website, url in websites.items():\n",
        "            t = threading.Thread(target=fetch_articles, args=(website, url))\n",
        "            t.start()\n",
        "            threads.append(t)\n",
        "\n",
        "        for t in threads:\n",
        "            t.join()  # Wait for all threads to complete or exit early\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"KeyboardInterrupt received. Stopping and saving current progress...\")\n",
        "        for t in threads:\n",
        "            t.join()  # Ensure all threads stop\n",
        "\n",
        "def main():\n",
        "    websites = {\n",
        "        'BBC': 'http://feeds.bbci.co.uk/news/rss.xml',\n",
        "        'CNN': 'http://rss.cnn.com/rss/edition.rss',\n",
        "        'The Guardian': 'https://www.theguardian.com/uk/rss',\n",
        "        'New York Times': 'https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml',\n",
        "        'Al Jazeera': 'https://www.aljazeera.com/xml/rss/all.xml'\n",
        "    }\n",
        "    controlScraping(websites)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3z_RIspRSPW",
        "outputId": "3d6ba200-b7e1-4b2e-c51c-400e3ff8647b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching articles from: http://feeds.bbci.co.uk/news/rss.xml\n",
            "Fetching articles from: http://rss.cnn.com/rss/edition.rss\n",
            "Fetching articles from: https://www.theguardian.com/uk/rss\n",
            "Fetching articles from: https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\n",
            "Fetching articles from: https://www.aljazeera.com/xml/rss/all.xml\n",
            "Fetched 25 articles from Al Jazeera. Total collected: 25\n",
            "Fetched 33 articles from BBC. Total collected: 33Fetched 25 articles from New York Times. Total collected: 25\n",
            "\n",
            "Fetched 50 articles from CNN. Total collected: 50\n",
            "Fetched 117 articles from The Guardian. Total collected: 117\n",
            "Fetched 50 articles from Al Jazeera. Total collected: 50\n",
            "Fetched 50 articles from New York Times. Total collected: 50\n",
            "Fetched 66 articles from BBC. Total collected: 66\n",
            "Fetched 100 articles from CNN. Total collected: 100\n",
            "Fetched 234 articles from The Guardian. Total collected: 234\n",
            "Fetched 75 articles from Al Jazeera. Total collected: 75\n",
            "Fetched 75 articles from New York Times. Total collected: 75\n",
            "Fetched 99 articles from BBC. Total collected: 99\n",
            "Fetched 150 articles from CNN. Total collected: 150\n",
            "Fetched 351 articles from The Guardian. Total collected: 351\n",
            "Fetched 100 articles from Al Jazeera. Total collected: 100\n",
            "Fetched 100 articles from New York Times. Total collected: 100\n",
            "Fetched 132 articles from BBC. Total collected: 132\n",
            "Fetched 200 articles from CNN. Total collected: 200\n",
            "Fetched 125 articles from Al Jazeera. Total collected: 125\n",
            "Fetched 468 articles from The Guardian. Total collected: 468\n",
            "Fetched 125 articles from New York Times. Total collected: 125\n",
            "Fetched 165 articles from BBC. Total collected: 165\n",
            "Fetched 250 articles from CNN. Total collected: 250\n",
            "Fetched 150 articles from Al Jazeera. Total collected: 150\n",
            "Fetched 150 articles from New York Times. Total collected: 150\n",
            "Fetched 198 articles from BBC. Total collected: 198\n",
            "Reached 500 articles for The Guardian.\n",
            "Fetched 500 articles from The Guardian. Total collected: 500\n",
            "Fetched 175 articles from Al Jazeera. Total collected: 175\n",
            "Fetched 300 articles from CNN. Total collected: 300\n",
            "Fetched 175 articles from New York Times. Total collected: 175\n",
            "Saved articles from The Guardian to The Guardian_articles.csv\n",
            "Fetched 231 articles from BBC. Total collected: 231\n",
            "Fetched 200 articles from Al Jazeera. Total collected: 200\n",
            "Fetched 350 articles from CNN. Total collected: 350\n",
            "Fetched 200 articles from New York Times. Total collected: 200\n",
            "Fetched 264 articles from BBC. Total collected: 264\n",
            "Fetched 225 articles from Al Jazeera. Total collected: 225\n",
            "Fetched 225 articles from New York Times. Total collected: 225\n",
            "Fetched 400 articles from CNN. Total collected: 400\n",
            "Fetched 297 articles from BBC. Total collected: 297\n",
            "Fetched 250 articles from Al Jazeera. Total collected: 250\n",
            "Fetched 250 articles from New York Times. Total collected: 250\n",
            "Fetched 450 articles from CNN. Total collected: 450\n",
            "Fetched 330 articles from BBC. Total collected: 330\n",
            "Fetched 275 articles from Al Jazeera. Total collected: 275\n",
            "Fetched 275 articles from New York Times. Total collected: 275\n",
            "Fetched 363 articles from BBC. Total collected: 363\n",
            "Fetched 500 articles from CNN. Total collected: 500\n",
            "Fetched 300 articles from Al Jazeera. Total collected: 300\n",
            "Fetched 300 articles from New York Times. Total collected: 300\n",
            "Fetched 396 articles from BBC. Total collected: 396\n",
            "Saved articles from CNN to CNN_articles.csv\n",
            "Fetched 325 articles from Al Jazeera. Total collected: 325\n",
            "Fetched 325 articles from New York Times. Total collected: 325\n",
            "Fetched 429 articles from BBC. Total collected: 429\n",
            "Fetched 350 articles from Al Jazeera. Total collected: 350\n",
            "Fetched 350 articles from New York Times. Total collected: 350\n",
            "Fetched 462 articles from BBC. Total collected: 462\n",
            "Fetched 375 articles from Al Jazeera. Total collected: 375\n",
            "Fetched 375 articles from New York Times. Total collected: 375\n",
            "Fetched 495 articles from BBC. Total collected: 495\n",
            "Fetched 400 articles from Al Jazeera. Total collected: 400\n",
            "Fetched 400 articles from New York Times. Total collected: 400\n",
            "Reached 500 articles for BBC.\n",
            "Fetched 500 articles from BBC. Total collected: 500\n",
            "Fetched 425 articles from Al Jazeera. Total collected: 425\n",
            "Fetched 425 articles from New York Times. Total collected: 425\n",
            "Saved articles from BBC to BBC_articles.csv\n",
            "Fetched 450 articles from Al Jazeera. Total collected: 450\n",
            "Fetched 450 articles from New York Times. Total collected: 450\n",
            "Fetched 475 articles from Al Jazeera. Total collected: 475\n",
            "Fetched 475 articles from New York Times. Total collected: 475\n",
            "Fetched 500 articles from Al Jazeera. Total collected: 500\n",
            "Fetched 500 articles from New York Times. Total collected: 500\n",
            "Saved articles from Al Jazeera to Al Jazeera_articles.csv\n",
            "Saved articles from New York Times to New York Times_articles.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}